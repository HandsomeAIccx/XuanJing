.. _psro:

PSRO
==========

在介绍PSRO算法之前,需要先介绍一点博弈论里面的基本概念。

博弈论基础知识
----------------

博弈论研究的前提假设是每个智能体是理性的。这个假设在实际生活中可能很难实现,
但是并不影响我们研究它。假设每个智能体是理性的这一假设,在演化博弈里面并没有。

经典的博弈
>>>>>>>>>>>>>>>>

1. 零和博弈(Zero-Sum Game): :math:`u_{1}(a) + u_{2}(a) = 0`.
2. 合作博弈(Cooperative Game): :math:`u_{i}(a) = u_{j}(a)\ \forall a \in A, i,j \in N`
3. 协同博弈(Coordination Game): 存在多个纳什均衡。
4. 社会困境(Social Dilemma): 能够找到一个纳什均衡, 但是往往不会选到这个纳什均衡。比如囚徒困境。

纯策略和混合策略
>>>>>>>>>>>>>>>>

假设有这样一个猜硬币的问题,猜硬币的正反面。如果是纯策略(pure strategy)的话,那么对于智能体1和智能体2,来说动作空间
只是所能选取动作集合中的一个 :math:`a_{1} \in A_{1} = \{Heads, Tails\}`,或者是
:math:`a_{2} \in A_{2} = \{Heads, Tails\}`.

而对于混合策略(mixed strategy)来说, 策略是按照一定的概率选取第一个,一定的概率选取第二个,数学
表示形式为: :math:`a_{1} = (x_{H}, x_{T}), x_{H} \in [0, 1], x_{H} + x_{T} = 1`,或者是
:math:`a_{2} = (y_{H}, y_{T}), y_{H} \in [0, 1], y_{H} + y_{T} = 1`,

此时对于智能体1和智能体2的期望收益分别为

智能体1:

:math:`EU_{1} = x_{H}y_{H}u_{1}(H, H) + x_{H}y_{T}u_{1}(H, T) + x_{T}y_{H}u_{1}(T, H) + x_{T}y_{T}u_{1}(T, T)`,

智能体2:

:math:`EU_{2} = x_{H}y_{H}u_{2}(H, H) + x_{H}y_{T}u_{2}(H, T) + x_{T}y_{H}u_{2}(T, H) + x_{T}y_{T}u_{2}(T, T)`,


正则式博弈(Normal-form Game)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>

定义一个博弈问题通常只需要三个要素即可：玩家(Players)、策略(Strategies)
和收益(Payoff)。玩家数学定义为 :math:`N = \{1, 2, \cdots, n\}`, 如果是两个
玩家,则 :math:`n=2`。策略也是动作,对于每一个玩家来说都有一个动作集合
:math:`A=A_{1} \times A_{2} \times \cdots \times`。例如像对于石头剪刀布
玩家1的动作集合和动作2的集合相等 :math:`A_{1} = A_{2} = \{R, S, P\}`.
对于收益矩阵 :math:`u = (u_{1}, u_{2}, \cdots, u_{n})` 来说,
每个玩家都有自己的收益矩阵,它是一个函数,对于石头剪刀布游戏来说,
输入是每个玩家分别采取的动作,输出是玩家的收益, 可以表示为
:math:`u_{i}: A \rightarrow \mathbb{R}`。

对于上述问题可以通过表格的形式将其列出来:

+------+------+------+------+
|      |  R   |   S  |   P  |
+======+======+======+======+
| R    | 0,0  | 1,-1 | -1,1 |
+------+------+------+------+
| S    | -1,1 |  0,0 |  1,-1|
+------+------+------+------+
| P    | 1-1  | -1,1 |  0,0 |
+------+------+------+------+

能通过表格形式表示出来的博弈,称之为正则式博弈(Normal-form game)。通常将
行玩家的收益写在前面,列玩家的收益写在后面。

扩展式博弈(Extensive-form Game)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

扩展式博弈通常用一颗博弈树来表示,这颗博弈树中有一些节点,有一些边。每个节点都是决策点,
边是玩家选取的动作。到了叶节点定义最终的博弈结果。

正则式博弈是单步的(single step), 也称之为静态的。扩展式博弈为多步的(multiple step),
也称之为动态的(dynamic).


如果历史的动作是对于对手不可见的,称之为非完美信息(Information Game)。

马尔科夫博弈(Markov Game)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

马尔科夫博弈或者称之为随机博弈(Stochastic Game). 在马尔科夫博弈问题中, 定义了
一个状态空间, 在这个状态空间 :math:`S` 中, 状态之间是可以随意跳动的。在扩展式博弈问题中, 基本上还是
按照博弈树展开的。 动作空间可以表示为
:math:`A = A_{1} \times A_{2} \cdots \times A_{n}`.
转移函数 :math:`p: S \times A \rightarrow S`,
奖励函数为 :math:`r: S \times A \rightarrow \mathbb{R}^{n}`.
行为策略依据状态选择动作,
`Policy` :math:`\pi_{i}: S \times A_{i} \rightarrow [0, 1]`.


贝叶斯博弈
>>>>>>>>>>


非完全信息(Incomplete Information): 博弈刚开始的时候,
是不是有人隐藏了一些只有他自己知道的信息。针对这种非完全信息的博弈,
我们用一个模型叫做贝叶斯博弈去刻画它。其实也就是用概率去估计隐藏的信息。

贝叶斯博弈的模型基本思想是: 既然不知道效用函数 :math:`p_{i}` 是什么,
我就用一个概率分布去建模它。在这个贝叶斯博弈里面,
我们假设每一个玩家他只知道一个概率分布。
这里有个专门的词叫type,其实就是玩家他的一些隐藏的信息,
可以认为这个空间,每一个空间都是这个玩家他隐藏信息的空间。

那么贝叶斯博弈的模型数学形式可以表示为: :math:`N = \{1, 2, \cdots, n\}`表示为玩家
如果是两个玩家,则 :math:`n=2`。策略也是动作,对于每一个玩家来说都有一个动作集合
:math:`A=A_{1} \times A_{2} \times \cdots \times`。 但是玩家有隐藏信息,
基于玩家隐藏信息的不同, 可以将玩家分为不同的类型,
:math:`\Theta = \Theta_{1} \times \Theta_{2} \cdots \Theta_{n}`.
此时收益矩阵 :math:`u = (u_{1}, u_{2}, \cdots, u_{n}), u_{i}:
\Theta \times A \rightarrow \mathbb{R}`. 对应到策略上面, 纯策略可以表示为
:math:`\pi_{i}:\Theta_{i} \rightarrow A_{i}`,
混合策略 :math:`\pi_{i}:\Theta_{i} \times A_{i}\rightarrow [0, 1]`


扩展式博弈结合贝叶斯博弈就变成了动态贝叶斯博弈。
动态贝叶斯博弈：每个玩家针对一个信息集,维护一个belief, 它是从状态到概率的一个映射。

纳什均衡(Nash Equilibrium)
--------------------------

纳什均衡的定义
>>>>>>>>>>>>>>>>>>>

在介绍纳什均衡之前, 我们需要先了解一个概念最佳应对(Best Response),在给定
其他人的动作 :math:`a_{-i} \in A_{1} \times \cdots, \times A_{i-1}
\times A_{i+1} \times \cdots \times A_{n}`. 最佳反应
:math:`a_{i} \rightarrow u(a_{i}, a_{-i})
\geq u(a_{i}^{\prime}, a_{-i}) \ \forall a_{i}^{\prime} \in A_{i}`.
最佳应对：给定对方某一个动作,己方所采取的这个动作所获得的效用最好。

占优策略(Dominant Strategy)：
对于我方某一个动作 :math:`a_{i}`,对方不管采用什么样的动作 :math:`a_{-i}`
,我方这个动作都是最佳应对。

纳什均衡是一个联合策略(joint strategy, strategy profile), 对于任意一个玩家
:math:`i`, :math:`a_{i}`是最优应对. 纳什均衡：
每一方都不能通过单纯的改变己方的策略,使得己方的情况变得更好。

但是上述均衡点并没有考虑状态转移函数(state transitions).
在马尔科夫博弈中均衡点是Markov Perfect Equilibrium.

均衡分析
>>>>>>>>>>>>>>>>>>>


对于两人零和博弈来说, 如果玩家一有 :math:`m` 种策略, 玩家二有
 :math:`n` 种策略, 以矩阵 :math:`A_{m \times n}` 表示玩家
一的PayOff;
由于是零和博弈, 玩家二的PayOff是 :math:`-A`.
设 :math:`p_{1 \times m}` 是玩家一的混合策略,
:math:`q_{1} \times n` 是玩家二的混合策略. 那么对于玩家一,
其混合策略应该是如下线性规划问题的解:

.. math::
    & v_{1} = max \ v \\
    &s.t. \ p A \geq v \cdot 1 \\
    &p_{1 \times m} \geq 0, p \times 1 = 1

对于玩家二, 其混合策略线性规划问题可以表示为:

.. math::
    & v_{2} = max \ v \\
    &s.t. \ -q A^{T} \geq v \cdot 1 \\
    &q_{1 \times m} \geq 0, q \times 1 = 1

记 :math:`w=-v`, 上面这个问题可以写作:

.. math::
    & -v_{2} = min \ w \\
    & s.t. \ q_{1 \times n} \geq 0, q \cdot 1 = 1 \\
    & q A^{T} \leq w \cdot 1


上述两个问题是一个对偶问题. 两人寻优
的过程互为对偶, 从而解决了该问题均衡的
唯一性, 计算复杂度等问题.

虚拟自博
>>>>>>>>>>>>>>


事实上在求解双人零和博弈问题上不仅有线性规划的方法,
还有Fictitious Play的方法. 虚拟自博是一种迭代的方式
虚拟自博的核心思想是说去记录对手的历史动作,
然后基于对手的历史动作计算最优反应, 然后将策略放到策略集
中去:

.. math::
    & a_{i}^{t^{*}} \in BR_{i}(p_{-i}^{t}=\frac{1}{t}
    \sum_{t=0}^{t-1}\mathcal{f}\{a_{-i}^{t}=a,a\in \mathbb{A}\}) \\
    & p_{i}^{t+1} = (1-\frac{1}{t})p_{i}^{t} + \frac{1}{t}a_{i}^{t},
    for \ all \ i.




帕累托最优
>>>>>>>>>>>>>>>>>>>


帕累托最优(Pareto Optimality),也称为帕累托效率(Pareto efficiency),
是指资源分配的一种理想状态,假定固有的一群人和可分配的资源,
从一种分配状态到另一种状态的变化中,在没有使任何人境况变坏的前提下,
使得至少一个人变得更好，这就是帕累托改进或帕累托最优化。
帕累托最优状态就是不可能再有更多的帕累托改进的余地.
换句话说,帕累托改进是达到帕累托最优的路径和方法.

子博弈纳什均衡
>>>>>>>>>>>>>>>>>>>


子博弈纳什均衡(Subgame Perfect Nash Equilibrium, SPNE)


Double Oracle
>>>>>>>>>>>>>>>>>


Double Oracle也是一种迭代式算法, 但是是在子博弈空间上
求解的. Double Oracle一定是会收敛的, 因为最差的情况就是
求解整个博弈问题.

如何证明子博弈问题会是原博弈问题的纳什均衡呢？
假设第 :math:`j` 个迭代中得到的纳什均衡是
:math:`p_{j}` 和:math:`q_{j}` . 因为第:math:`j` 个迭代
已经停掉, 所以无法再找到最优反应.

此时,有:

.. math::
    \forall p, V(p, q_{j}) \geq v \rightarrow \ \forall p, max V(p,q) \geq v \\
    \forall q, V(p_{j}, q) \leq v \rightarrow \ max_{q} V(p_{j},q) \leq v

:math:`p_{j}`一定会是minimax最优.

贝叶斯纳什均衡
>>>>>>>>>>>>>>>>>>>


纳什均衡的理论结果
>>>>>>>>>>>>>>>>>>>


在纳什均衡中, 有三个需要注意的地方, 一个是如何计算纳什均衡,一个是均衡的效率,
另一个是机制设计. 目前的多智能体强化学习主要还是聚焦在如何计算均衡点.

PSRO
----------


如果在Double Oracle过程中Best Response换成
强化学习的算法时, 就变成了(Policy Space
Response Oracle, PSRO), 也就是在策略空间
去找最佳反应.




重复博弈和学习方法(Repeated Game)
-----------------------------------


进化博弈理论和联盟博弈
-----------------------



进化博弈理论(Evolutionary Game Theory)和联盟博弈(Coalitional Game Theory)

进化博弈论中的Replicator Dynamics

进化博弈论,注重研究演化的过程,而不是状态,刻画博弈策略的变化过程